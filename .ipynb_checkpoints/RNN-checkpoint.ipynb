{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97a04e9-7365-41b5-a711-e25ce1a0613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ã¯', 'JJ'), ('START', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('PROJECT', 'NNP'), ('GUTENBERG', 'NNP'), ('EBOOK', 'NNP'), ('ALICE', 'NNP'), ('ADVENTURES', 'NNP'), ('IN', 'NNP'), ('WONDERLAND', 'NNP'), ('Illustration', 'NNP'), ('Adventures', 'NNP'), ('Wonderland', 'NNP'), ('Lewis', 'NNP'), ('Carroll', 'NNP'), ('THE', 'NNP'), ('MILLENNIUM', 'NNP'), ('FULCRUM', 'NNP'), ('EDITION', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import warnings\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "alice_text = requests.get(url).text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "alice_tokens = preprocess_text(alice_text)\n",
    "alice_pos_tags = pos_tag(alice_tokens)\n",
    "print(alice_pos_tags[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c0f48a-274e-48ae-b46f-1a7e78269ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_dataset(pos_tags, n_context=3):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(n_context, len(pos_tags) - n_context):\n",
    "        context = [pos_tags[j][0] for j in range(i-n_context, i+n_context+1) if j != i]\n",
    "        target = pos_tags[i][1]\n",
    "\n",
    "        X.append(context)\n",
    "        Y.append(target)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = create_dataset(alice_pos_tags)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "train_X, train_Y = X[:train_size], Y[:train_size]\n",
    "val_X, val_Y = X[train_size:], Y[train_size:]\n",
    "\n",
    "train_data = list(zip(train_X, train_Y))\n",
    "random.shuffle(train_data)\n",
    "train_X, train_Y = zip(*train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39245dd5-c3fe-41fe-9498-baefb2cef34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "sequence_length = 7\n",
    "sequences = []\n",
    "for i in range(sequence_length, len(alice_pos_tags) - sequence_length + 1):\n",
    "    seq = alice_pos_tags[i-sequence_length:i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "\n",
    "words = [[w[0] for w in seq] for seq in sequences]\n",
    "tags = [seq[3][1] for seq in sequences]\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(words)\n",
    "word_sequences = word_tokenizer.texts_to_sequences(words)\n",
    "\n",
    "unique_tags = list(set(tags))\n",
    "tag_index = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "tags_encoded = [tag_index[tag] for tag in tags]\n",
    "tags_encoded = to_categorical(tags_encoded, num_classes=len(unique_tags))\n",
    "\n",
    "max_seq_length = max(len(seq) for seq in word_sequences)\n",
    "word_sequences_padded = pad_sequences(word_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(word_sequences_padded, tags_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f10e908-9d91-4239-9ba8-a32fc0695258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "302/302 [==============================] - 14s 23ms/step - loss: 2.5752 - accuracy: 0.1902 - val_loss: 2.4106 - val_accuracy: 0.2213\n",
      "Epoch 2/15\n",
      "302/302 [==============================] - 5s 15ms/step - loss: 2.3119 - accuracy: 0.2572 - val_loss: 2.2702 - val_accuracy: 0.2682\n",
      "Epoch 3/15\n",
      "302/302 [==============================] - 5s 17ms/step - loss: 2.0865 - accuracy: 0.3168 - val_loss: 2.1680 - val_accuracy: 0.2914\n",
      "Epoch 4/15\n",
      "302/302 [==============================] - 5s 18ms/step - loss: 1.8651 - accuracy: 0.3848 - val_loss: 2.0829 - val_accuracy: 0.3205\n",
      "Epoch 5/15\n",
      "302/302 [==============================] - 5s 16ms/step - loss: 1.6608 - accuracy: 0.4499 - val_loss: 2.0324 - val_accuracy: 0.3611\n",
      "Epoch 6/15\n",
      "302/302 [==============================] - 5s 17ms/step - loss: 1.4393 - accuracy: 0.5255 - val_loss: 1.9512 - val_accuracy: 0.3944\n",
      "Epoch 7/15\n",
      "302/302 [==============================] - 6s 18ms/step - loss: 1.2029 - accuracy: 0.6127 - val_loss: 1.9277 - val_accuracy: 0.4400\n",
      "Epoch 8/15\n",
      "302/302 [==============================] - 5s 16ms/step - loss: 0.9424 - accuracy: 0.6992 - val_loss: 1.7839 - val_accuracy: 0.4981\n",
      "Epoch 9/15\n",
      "302/302 [==============================] - 5s 17ms/step - loss: 0.7375 - accuracy: 0.7707 - val_loss: 1.7445 - val_accuracy: 0.5176\n",
      "Epoch 10/15\n",
      "302/302 [==============================] - 5s 18ms/step - loss: 0.5509 - accuracy: 0.8345 - val_loss: 1.7753 - val_accuracy: 0.5401\n",
      "Epoch 11/15\n",
      "302/302 [==============================] - 5s 17ms/step - loss: 0.3981 - accuracy: 0.8789 - val_loss: 1.7636 - val_accuracy: 0.5812\n",
      "Epoch 12/15\n",
      "302/302 [==============================] - 5s 17ms/step - loss: 0.3072 - accuracy: 0.9099 - val_loss: 1.7614 - val_accuracy: 0.6032\n",
      "Epoch 13/15\n",
      "302/302 [==============================] - 6s 18ms/step - loss: 0.2435 - accuracy: 0.9276 - val_loss: 1.7396 - val_accuracy: 0.6272\n",
      "Epoch 14/15\n",
      "302/302 [==============================] - 5s 16ms/step - loss: 0.1877 - accuracy: 0.9447 - val_loss: 1.8215 - val_accuracy: 0.6372\n",
      "Epoch 15/15\n",
      "302/302 [==============================] - 5s 16ms/step - loss: 0.1558 - accuracy: 0.9555 - val_loss: 1.8494 - val_accuracy: 0.6343\n",
      "76/76 [==============================] - 0s 6ms/step - loss: 1.8494 - accuracy: 0.6343\n",
      "Validation Accuracy: 0.6342880725860596\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, TimeDistributed\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "embed_size = 128\n",
    "lstm_units = 96\n",
    "num_classes = y_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, input_length=max_seq_length))\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=32)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba29f43-64b8-4110-a24b-3836863f71a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
